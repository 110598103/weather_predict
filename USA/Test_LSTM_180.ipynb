{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5f579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Lambda,Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "from tensorflow.keras.models import Sequential  #用來啟動 NN\n",
    "from tensorflow.keras.layers import Conv2D  # Convolution Operation\n",
    "from tensorflow.keras.layers import MaxPooling2D # Pooling\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense # Fully Connected Networks\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate, AveragePooling2D, UpSampling2D, add, Multiply, GlobalAveragePooling2D\n",
    "import tensorflow.keras as keras  \n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb6fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(Path):\n",
    "    files=os.listdir(Path)\n",
    "    random.shuffle(files)\n",
    "    Train_Lstm=[]\n",
    "    Label=[]\n",
    "    Train=[]\n",
    "    Train\n",
    "    count=1\n",
    "    Train_Pdf=[]\n",
    "    Time_lists = []\n",
    "    for file in files:\n",
    "        Time_lists.append(file)\n",
    "        start=time.time()\n",
    "        Break=0\n",
    "        global Train_pdf, Label_Pdf\n",
    "        Train_lstm=[]\n",
    "        file_data=os.listdir(Path+file)\n",
    "        \n",
    "        ##CNN_PDF\n",
    "        #CSFV2\n",
    "        df_csfv2 = pd.read_csv(Path+file+'/'+file_data[0],header=None)\n",
    "        df_era5 = pd.read_csv(Path+file+'/'+file_data[1],header=None)\n",
    "        for column in df_csfv2.columns[2:12]:\n",
    "            if column == df_csfv2.columns[2]:\n",
    "                Train_pdf=np.array(np.array(df_csfv2[column]).reshape(61,111,1))\n",
    "            else:\n",
    "                Train_pdf=np.append(Train_pdf,np.array(df_csfv2[column]).reshape(61,111,1),axis=2)\n",
    "                \n",
    "        #ERAF\n",
    "        for column in df_era5.columns[2:7]:\n",
    "            Train_pdf=np.append(Train_pdf,np.array(df_era5[column]).reshape(61,111,1),axis=2)\n",
    "\n",
    "        ##LSTM\n",
    "        #ERA5\n",
    "        df_1 = pd.read_csv(Path+file+'/'+file_data[3])\n",
    "        for column in df_1.columns[1:91]:\n",
    "            if' NaN' in list(df_1[column])[:]:\n",
    "                Break=1\n",
    "                break\n",
    "            Train_lstm.append(list(df_1[column]))\n",
    "            \n",
    "        #CSFV2\n",
    "        df_2 = pd.read_csv(Path+file+'/'+file_data[2])\n",
    "        for column in df_2.columns[1:181]:\n",
    "            if' NaN' in list(df_2[column])[:]:\n",
    "                Break=1\n",
    "                break\n",
    "            Train_lstm.append(list(df_2[column]))\n",
    "        if Break==1:\n",
    "            continue\n",
    "\n",
    "        ##LABEL_PDF \n",
    "        for column in df_era5.columns[7:17]:\n",
    "            if column == df_era5.columns[7]:\n",
    "                Label_Pdf=np.array(np.array(df_era5[column]).reshape(61,111,1))\n",
    "                #print(Label_Pdf)\n",
    "            else:\n",
    "                Label_Pdf=np.append(Label_Pdf,np.array(df_era5[column]).reshape(61,111,1),axis=2)\n",
    "\n",
    "        Train_Pdf.append(Train_pdf)\n",
    "        Train_Lstm.append(Train_lstm)\n",
    "        Label.append(Label_Pdf)\n",
    "        end=time.time()\n",
    "        Time=(end-start)*(len(files)-count)\n",
    "        print('%d / %d , Time : %d : %d : %d'%(count,len(files),int(Time/3600),int(Time%3600/60),Time%3600%60),end='\\r')\n",
    "        count=count+1\n",
    "    Train.append(Train_Pdf)\n",
    "    Train.append(Train_Lstm)\n",
    "    Label=np.array(Label)\n",
    "    return Train, Label, Time_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料位置\n",
    "Path='D:/weather/data_180/Test/'\n",
    "#Path='D:/weather/data_180/Train/'\n",
    "\n",
    "#載入資料\n",
    "Valid,Valid_Label,Time_lists=data_collect(Path)\n",
    "\n",
    "#模型位置\n",
    "model_path = 'D:/weather/model/LSMT_Restnet_new_data_my_loss_only_LSTM_180days.h5'\n",
    "\n",
    "#預測CSV儲存位置\n",
    "Path_perdict_csv = 'D:/weather/Predict_csv/LSTM_180/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_Pdf=Valid[0]\n",
    "Valid_Lstm=Valid[1]\n",
    "Valid_Pdf=np.array(Valid_Pdf)\n",
    "Valid_Lstm=np.array(Valid_Lstm)\n",
    "print('Valid_Lstm:',Valid_Lstm.shape)\n",
    "print('Train_Pdf:',Valid_Pdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcustom_loss(y_true, y_pred):\n",
    "\n",
    "    trueAvg = (y_true[:,:,:,0][:,:,:,np.newaxis]-Mean[0])/(Max[0]-Min[0])\n",
    "    trueStd = (y_true[:,:,:,1][:,:,:,np.newaxis]-Mean[1])/(Max[1]-Min[1])\n",
    "    trueMax = (y_true[:,:,:,2][:,:,:,np.newaxis]-Mean[2])/(Max[2]-Min[2])\n",
    "    trueMin = (y_true[:,:,:,3][:,:,:,np.newaxis]-Mean[3])/(Max[3]-Min[3])\n",
    "    trueSkw = (y_true[:,:,:,4][:,:,:,np.newaxis]-Mean[4])/(Max[4]-Min[4])\n",
    "\n",
    "    predAvg = (y_pred[:,:,:,0][:,:,:,np.newaxis]-Mean[0])/(Max[0]-Min[0])\n",
    "    predStd = (y_pred[:,:,:,1][:,:,:,np.newaxis]-Mean[1])/(Max[1]-Min[1])\n",
    "    predMax = (y_pred[:,:,:,2][:,:,:,np.newaxis]-Mean[2])/(Max[2]-Min[2])\n",
    "    predMin = (y_pred[:,:,:,3][:,:,:,np.newaxis]-Mean[3])/(Max[3]-Min[3])\n",
    "    predSkw = (y_pred[:,:,:,4][:,:,:,np.newaxis]-Mean[4])/(Max[4]-Min[4])\n",
    "    \n",
    "    trueAvg_180 = (y_true[:,:,:,5][:,:,:,np.newaxis]-Mean[5])/(Max[5]-Min[5])\n",
    "    trueStd_180 = (y_true[:,:,:,6][:,:,:,np.newaxis]-Mean[6])/(Max[6]-Min[6])\n",
    "    trueMax_180 = (y_true[:,:,:,7][:,:,:,np.newaxis]-Mean[7])/(Max[7]-Min[7])\n",
    "    trueMin_180 = (y_true[:,:,:,8][:,:,:,np.newaxis]-Mean[8])/(Max[8]-Min[8])\n",
    "    trueSkw_180 = (y_true[:,:,:,9][:,:,:,np.newaxis]-Mean[9])/(Max[9]-Min[9])\n",
    "\n",
    "    predAvg_180 = (y_pred[:,:,:,5][:,:,:,np.newaxis]-Mean[5])/(Max[5]-Min[5])\n",
    "    predStd_180 = (y_pred[:,:,:,6][:,:,:,np.newaxis]-Mean[6])/(Max[6]-Min[6])\n",
    "    predMax_180 = (y_pred[:,:,:,7][:,:,:,np.newaxis]-Mean[7])/(Max[7]-Min[7])\n",
    "    predMin_180 = (y_pred[:,:,:,8][:,:,:,np.newaxis]-Mean[8])/(Max[8]-Min[8])\n",
    "    predSkw_180 = (y_pred[:,:,:,9][:,:,:,np.newaxis]-Mean[9])/(Max[9]-Min[9])\n",
    "\n",
    "    loss = ((K.square(predMin - trueMin) + K.square(predMax - trueMax) + K.square(predSkw - trueSkw) +\n",
    "            K.square(predAvg - trueAvg) + K.square(predStd - trueStd))+\n",
    "            (K.square(predMin_180 - trueMin_180) + K.square(predMax_180 - trueMax_180) + K.square(predSkw_180 - trueSkw_180) +\n",
    "            K.square(predAvg_180 - trueAvg_180) + K.square(predStd_180 - trueStd_180)))/10\n",
    "    loss = K.mean(loss, axis=-1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9685bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max=[307.9367, 17.7063, 313.7215, 304.4253, 3.8382,310.1568,18.0352,314.7128,306.1226,2.7172]\n",
    "Min=[232.6118, 0.1771, 244.0102, 204.4635, -4.7362,234.9248,0.1928,248.2585,220.1559,-4.1973]\n",
    "Mean=[287.3242734287198,\n",
    "     3.349621542723795,\n",
    "     293.65658980295865,\n",
    "     279.6666574399456,\n",
    "     -0.25665387472126,\n",
    "     288.3584796428863,\n",
    "     3.2974777666300223,\n",
    "     294.7044402373835,\n",
    "     281.08416123969715,\n",
    "     -0.15500007492913936]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = load_model(model_path,custom_objects={'mcustom_loss': mcustom_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize=1\n",
    "y_predict=model.predict(Valid_Lstm,batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in range(len(y_predict)):\n",
    "    CSV = []\n",
    "    for i in range(61):\n",
    "        for j in range(111):\n",
    "            CSV_PER = []\n",
    "            CSV_PER.append(i+1)\n",
    "            CSV_PER.append(j+1)\n",
    "            for k in y_predict[day][i][j]:\n",
    "                CSV_PER.append(k)\n",
    "            CSV.append(CSV_PER)\n",
    "    df= pd.DataFrame(CSV,columns=['X','Y','Mean_90','STD_90','Maximum_90','Minimum_90','Skewness_90','Mean_180','STD_180','Maximum_180','Minimum_180','Skewness_180'])\n",
    "    df.to_csv(Path_perdict_csv+Time_lists[day]+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDF_point(i, j, data, start, end, Pic, DATE):\n",
    "    PDF=[0,0,0,0,0,0,0,0,0,0]\n",
    "    PDF_csfv2=[0,0,0,0,0,0,0,0,0,0]\n",
    "    LABEL = ['Mean_90','STD_90','Maximum_90','Minimum_90','Skewness_90','Mean_180','STD_180','Maximum_180','Minimum_180','Skewness_180']\n",
    "    Era5=[]\n",
    "    Csfv2=[]\n",
    "    Predict=[]\n",
    "    print('第'+str(Pic+1)+'天:',DATE)\n",
    "    print(\"Point:\",i+1,j+1)\n",
    "    DATA = []\n",
    "    for pdf in range(start, end):                \n",
    "        count=Valid_Label[Pic][i][j][pdf]\n",
    "        count_data=data[i][j][pdf]\n",
    "        count_csfv2=Valid_Pdf[Pic][i][j][5+pdf]\n",
    "        Era5.append(round(count,2))\n",
    "        Csfv2.append(round(count_csfv2,2))\n",
    "        Predict.append(round(count_data,2))\n",
    "        #print(Pic+1,pdf,'ERA5:',round(count,2),\"CSFV2:\",round(count_csfv2,2),'Predict:',round(count_data,2))\n",
    "        DATA.append([LABEL[pdf],round(count,2),round(count_csfv2,2),round(count_data,2),round(count,2)-round(count_csfv2,2),round(count,2)-round(count_data,2)])\n",
    "    df = pd.DataFrame(DATA,columns=['Type','ERA5','CSFV2','Predict','Ori_diff','Pre_deff'])\n",
    "    print(df)\n",
    "    Era5_pdf = np.arange(Era5[3],Era5[2],0.1)\n",
    "    Csfv2_pdf = np.arange(Csfv2[3],Csfv2[2],0.1)\n",
    "    Predict_pdf = np.arange(Predict[3],Predict[2],0.1)\n",
    "    plt.plot(Era5_pdf, norm.pdf(Era5_pdf, Era5[0], Era5[1]),label='ERA5' )\n",
    "    plt.plot(Csfv2_pdf, norm.pdf(Csfv2_pdf, Csfv2[0], Csfv2[1]),label='CSFV2' )\n",
    "    plt.plot(Predict_pdf, norm.pdf(Predict_pdf, Predict[0], Predict[1]),label='Predict' )\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7097576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDF_average(data, start, end, Pic, DATE):\n",
    "    PDF=[0,0,0,0,0,0,0,0,0,0]\n",
    "    PDF_csfv2=[0,0,0,0,0,0,0,0,0,0]\n",
    "    LABEL = ['Mean_90','STD_90','Maximum_90','Minimum_90','Skewness_90','Mean_180','STD_180','Maximum_180','Minimum_180','Skewness_180']\n",
    "    Era5=[]\n",
    "    Csfv2=[]\n",
    "    Predict=[]\n",
    "    print('第'+str(Pic+1)+'天:',DATE)\n",
    "    DATA = []\n",
    "    for pdf in range(start,end):\n",
    "        count=0\n",
    "        count_data=0\n",
    "        count_csfv2=0\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[0])):\n",
    "                count=count+Valid_Label[Pic][i][j][pdf]\n",
    "                count_data=count_data+data[i][j][pdf]\n",
    "                count_csfv2=count_csfv2+Valid_Pdf[Pic][i][j][5+pdf]\n",
    "        #print(LABEL[pdf],'ERA5:',round(count/(61*111),2),\"CSFV2:\",round(count_csfv2/(61*111),2),'Predict:',round(count_data/(61*111),2))\n",
    "        DATA.append([LABEL[pdf],round(count/(61*111),2),round(count_csfv2/(61*111),2),round(count_data/(61*111),2),round(count/(61*111),2)-round(count_csfv2/(61*111),2),round(count/(61*111),2)-round(count_data/(61*111),2)])\n",
    "        Era5.append(round(count/(61*111),2))\n",
    "        Csfv2.append(round(count_csfv2/(61*111),2))\n",
    "        Predict.append(round(count_data/(61*111),2))\n",
    "    df = pd.DataFrame(DATA,columns=['Type','ERA5','CSFV2','Predict','Ori_diff','Pre_deff'])\n",
    "    print(df)\n",
    "    Era5_pdf = np.arange(Era5[3],Era5[2],0.1)\n",
    "    Csfv2_pdf = np.arange(Csfv2[3],Csfv2[2],0.1)\n",
    "    Predict_pdf = np.arange(Predict[3],Predict[2],0.1)\n",
    "    plt.plot(Era5_pdf, norm.pdf(Era5_pdf, Era5[0], Era5[1]),label='ERA5' )\n",
    "    plt.plot(Csfv2_pdf, norm.pdf(Csfv2_pdf, Csfv2[0], Csfv2[1]),label='CSFV2' )\n",
    "    plt.plot(Predict_pdf, norm.pdf(Predict_pdf, Predict[0], Predict[1]),label='Predict' )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705be9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#單點作圖PDF\n",
    "Pic=0\n",
    "for data in y_predict:\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            \n",
    "            #前90天\n",
    "            print('前90天')\n",
    "            PDF_point(i, j, data, 0, 5, Pic)\n",
    "            \n",
    "            #後90天\n",
    "            print('後90天')\n",
    "            PDF_point(i, j, data, 5, 10, Pic)\n",
    "            \n",
    "        Pic=Pic+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0623ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#平均作圖PDF\n",
    "Pic=0\n",
    "for data in y_predict:\n",
    "    \n",
    "    #前90天\n",
    "    print('前90天')\n",
    "    PDF_average(data, 0, 5, Pic)\n",
    "    \n",
    "    #後90天\n",
    "    print('後90天')\n",
    "    PDF_average(data, 5, 10, Pic)\n",
    "    \n",
    "    Pic=Pic+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
